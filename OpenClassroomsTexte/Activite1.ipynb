{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activité 1 - Analysez vos données textuelles\n",
    "\n",
    "NB : J'ai dû lancer jupyter avec la commande suivante :<br>\n",
    "jupyter notebook --NotebookApp.iopub_data_rate_limit=1.0e10\n",
    "\n",
    "Ceci pour éviter d'avoir l'erreur décrite ici: <br>\n",
    "https://stackoverflow.com/questions/43288550/iopub-data-rate-exceeded-in-jupyter-notebook-when-viewing-image\n",
    "\n",
    "\n",
    "### Objectif\n",
    "Le but de cette activité est de nettoyer les données textuelles brutes fournies, et de créer un jeu de données d’entraînement en vue de créer un moteur de résumé automatique. \n",
    "\n",
    "Le jeu de données est téléchargeable ici: https://drive.google.com/uc?export=download&id=0BwmD_VLjROrfTHk4NFg2SndKcjQ\n",
    "\n",
    "\n",
    "### Contexte\n",
    "Les données brutes représentent un corpus d’articles CNN. L’objectif est de récupérer les features des documents et les highlights (résumés courts) associés concaténés, en vue d’entraîner un potentiel modèle de création de résumé d’articles.\n",
    "\n",
    "### Consigne\n",
    "Les opérations de traitement suivantes sont attendues sur le texte, pas forcément dans cet ordre :\n",
    "* Créer des paires de document (article, highlights)\n",
    "* Suppression de la ponctuation\n",
    "* Séparation en token en minuscules\n",
    "* Suppression des stopwords pour les articles\n",
    "* Calcul des fréquences et tf-idf sur les deux types de documents\n",
    "* Enregistrement du nouveau jeu de données d’entraînement pour usage ultérieur\n",
    "\n",
    "### Etapes\n",
    "#### 1. Lecture des données et création de paires (article, highlights).\n",
    "* On passe le texte en minuscules dès cette étape.\n",
    "   \n",
    "   \n",
    "#### 2. Tokenization des articles et des highlights.\n",
    "* Les stopwords sont conservés pour les highlights, mais retirés pour les articles. \n",
    "* Les deux corpus tokenizés sont stockés dans des listes.    \n",
    "* L'utilisation d'un RegexpTokenizer permet d'obtenir des tokens sans ponctuation.\n",
    "   \n",
    "   \n",
    "#### 3. Calcul des fréquences et tf-idf sur les deux types de documents.\n",
    "* Création de deux matrices de fréquences, une pour les articles et une pour les highlights, dénombrant le nombres d'occurrences de chaque mot dans chaque document.\n",
    "* Création de deux matrices tf-idf, une pour les articles et une pour les highlights, stockant le tf-idf de chaque paire (document, mot). \n",
    "* Création de deux dictionnaires, un pour les articles et un pour les highlights, permettant de mapper les mots dénombrés dans les matrices ci-dessus à leur représentation sous forme d'entier. \n",
    "   \n",
    "   \n",
    "#### 4. Stockage des listes et dictionnaires créés dans un fichier pickle. \n",
    "* Stockage des listes tokenizées, matrices de fréquence et tf-idf dans un fichier pickle.\n",
    "* On les recharge juste après pour vérifier que tout fonctionne. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Lecture des données et création de paires (article, highlights).\n",
    "* Lecture du jeu d'entraînement.\n",
    "* Création de deux listes, *articles* et *highlights*, contenant respectivement les articles et les résumés des fichiers du dataset passés en minuscules.\n",
    "* Création d'une liste *pairs* contenant des tuples (article, highlight), constitués à partir des index de chaque texte dans les listes *articles* et *highlights*. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def load_dataset(directory, articles, highlights, pairs):\n",
    "    \"\"\"\n",
    "    Browse the directory containing the dataset files.\n",
    "    Load each file content in memory. \n",
    "    Replace all upper case letters with lower case letters.\n",
    "    Add: \n",
    "      - The initial text in lower case to the articles list\n",
    "      - The highlights to the highlights list\n",
    "      - The pair of indexes to the pairs list\n",
    "    \"\"\"\n",
    "\n",
    "    for entry in os.listdir(directory):\n",
    "        \n",
    "        with open(directory + '/' + entry, 'r') as file:\n",
    "            \n",
    "            observation = file.read().lower()\n",
    "            splitted = observation.split(\"@highlight\")\n",
    "            articles.append(splitted[0])\n",
    "            article_index = len(articles) - 1\n",
    "            \n",
    "            for highlight in splitted[1:]:\n",
    "                highlights.append(highlight)\n",
    "                highlight_index = len(highlights) - 1\n",
    "                pairs.append((article_index, highlight_index))           \n",
    "                    \n",
    "articles = []\n",
    "highlights = []\n",
    "pairs = []\n",
    "load_dataset('cnn/stories', articles, highlights, pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Tokenization des articles et des highlights.\n",
    "* Les stopwords sont conservés pour les highlights, mais retirés pour les articles. \n",
    "* Les deux corpus tokenizés sont stockés dans des listes: *tokenized_articles* et *tokenized_highlights*    \n",
    "* L'utilisation d'un RegexpTokenizer permet d'obtenir des tokens sans ponctuation.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "stop_words = set(stopwords.words('english')) \n",
    " \n",
    "def preprocess(text, remove_stopwords=False):\n",
    "    \"\"\"\n",
    "    Suppress all punctuation in the given text. \n",
    "    Remove the stopwords if remove_stopwords is True.\n",
    "    \"\"\"\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokenized = tokenizer.tokenize(text)\n",
    "    \n",
    "    if remove_stopwords is True:\n",
    "        tokenized = [w for w in tokenized if w not in stop_words]\n",
    "    \n",
    "    return tokenized\n",
    "\n",
    "tokenized_articles = [preprocess(a, remove_stopwords=True) for a in articles]\n",
    "tokenized_highlights = [preprocess(h) for h in highlights]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Calcul des fréquences et tf-idf sur les deux types de documents.\n",
    "* Création de deux matrices de fréquences, *counts_articles* et *counts_highlights*, dénombrant le nombres d'occurrences de chaque mot dans chaque document.\n",
    "* Création de deux matrices tf-idf, *tfidf_articles* et *tfidf_highlights*, stockant le tf-idf de chaque paire (document, mot). \n",
    "* Création de deux dictionnaires, *vocabulary_articles* et *vocabulary_highlights*, permettant de mapper les mots dénombrés dans les matrices ci-dessus à leur représentation sous forme d'entier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import re\n",
    "\n",
    "def find_frequency(text_list, vectorizer):\n",
    "    \"\"\"\n",
    "    Apply the given vectorizer to the text_list.\n",
    "    \"\"\"\n",
    "    # Remove punctuation.\n",
    "    punctuation = re.compile('\\W+')\n",
    "    text_list = [punctuation.sub(r' ', text) for text in text_list]\n",
    "    \n",
    "    # Create a dictionary containing the count of each word of the vocabulary.\n",
    "    word_count_vector = vectorizer.fit_transform(text_list)\n",
    "    return word_count_vector\n",
    "\n",
    "# Find the count of each word in the articles and highlights corpuses\n",
    "cv_articles = CountVectorizer(stop_words=stop_words)\n",
    "cv_highlights = CountVectorizer()\n",
    "\n",
    "counts_articles = find_frequency(articles, cv_articles)\n",
    "counts_highlights = find_frequency(highlights, cv_highlights)\n",
    "\n",
    "# Find the tfidf for both the articles and highlights corpuses\n",
    "tv_articles = TfidfVectorizer(stop_words=stop_words)\n",
    "tv_highlights = TfidfVectorizer()\n",
    "\n",
    "tfidf_articles = find_frequency(articles, tv_articles)\n",
    "tfidf_highlights = find_frequency(highlights, tv_highlights)\n",
    "\n",
    "# Find the vocabulary dictionaries for each corpus\n",
    "vocabulary_articles = cv_articles.vocabulary_\n",
    "vocabulary_highlights = cv_highlights.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Stockage des listes et dictionnaires créés dans un fichier pickle. \n",
    "* Stockage des listes tokenizées, matrices de fréquence et tf-idf dans un fichier pickle.\n",
    "* On les recharge juste après pour vérifier que tout fonctionne. \n",
    "* On affiche un extrait de la matrice *counts_articles*, contenant, pour chaque couple (*document*, *word*), le nombre d'occurrences du mot *word* dans le document *document*.\n",
    "* On affiche un extrait de la matrice *tfidf_highlights*, contenant, pour chaque couple (*document*, *word*), le tf-idf du mot *word* dans le document *document*.\n",
    "* On vérifie sur un exemple que les paires (article, highlight) sont cohérentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the dataset to a pickle file\n",
    "with open('data_activite1.pickle', 'wb') as pickle_file:\n",
    "    \n",
    "    pickle.dump(pairs, pickle_file)\n",
    "    \n",
    "    pickle.dump(tokenized_articles, pickle_file)\n",
    "    pickle.dump(tokenized_highlights, pickle_file)\n",
    "    \n",
    "    pickle.dump(vocabulary_articles, pickle_file)\n",
    "    pickle.dump(vocabulary_highlights, pickle_file)\n",
    "    \n",
    "    pickle.dump(counts_articles, pickle_file)\n",
    "    pickle.dump(counts_highlights, pickle_file)\n",
    "    \n",
    "    pickle.dump(tfidf_articles, pickle_file)\n",
    "    pickle.dump(tfidf_highlights, pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dumped elements are loaded back into memory to check they were properly stored.\n",
    "with open('data_activite1.pickle', 'rb') as pickle_file:\n",
    "       \n",
    "    pairs = pickle.load(pickle_file)\n",
    "    \n",
    "    tokenized_articles = pickle.load(pickle_file)\n",
    "    tokenized_highlights = pickle.load(pickle_file)\n",
    "    \n",
    "    vocabulary_articles = pickle.load(pickle_file)\n",
    "    vocabulary_highlights = pickle.load(pickle_file)\n",
    "    \n",
    "    counts_articles = pickle.load(pickle_file)\n",
    "    counts_highlights = pickle.load(pickle_file)\n",
    "    \n",
    "    tfidf_articles = pickle.load(pickle_file)\n",
    "    tfidf_highlights = pickle.load(pickle_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SANITY CHECK 1\n",
      "\n",
      "Here are 10 tuples (A, W) with the number of times the word W appears in the article A:\n",
      "     The article 76035  contains 2   time(s) the word different\n",
      "     The article 65611  contains 3   time(s) the word wednesday\n",
      "     The article 84380  contains 1   time(s) the word main    \n",
      "     The article 69580  contains 7   time(s) the word debate  \n",
      "     The article 44328  contains 1   time(s) the word random  \n",
      "     The article 64578  contains 1   time(s) the word concealed\n",
      "     The article 20446  contains 1   time(s) the word operate \n",
      "     The article 30427  contains 1   time(s) the word dragging\n",
      "     The article 27195  contains 1   time(s) the word dramatic\n",
      "     The article 34843  contains 1   time(s) the word pin     \n",
      "\n",
      "SANITY CHECK 2\n",
      "\n",
      "Here are 10 tuples (H, W) with the tfidf of the word W in the highlight H:\n",
      "     The word problem         in the highlight 2585     has a tf-idf of 0.27244081862497094.\n",
      "     The word off             in the highlight 261051   has a tf-idf of 0.24076878440981195.\n",
      "     The word explosion       in the highlight 221503   has a tf-idf of 0.281852793489314.\n",
      "     The word of              in the highlight 31493    has a tf-idf of 0.08829172354296638.\n",
      "     The word france          in the highlight 244983   has a tf-idf of 0.37111701719766493.\n",
      "     The word festival        in the highlight 234522   has a tf-idf of 0.32334556830913874.\n",
      "     The word has             in the highlight 93370    has a tf-idf of 0.1540007890131705.\n",
      "     The word stops           in the highlight 157249   has a tf-idf of 0.3521414322609076.\n",
      "     The word after           in the highlight 36259    has a tf-idf of 0.23148592640974322.\n",
      "     The word the             in the highlight 166487   has a tf-idf of 0.08757402528802545.\n",
      "\n",
      "SANITY CHECK 3\n",
      "\n",
      "The following highlight sums up the following tokenized article:\n",
      "\n",
      "Highlight:\n",
      "more than 1 000 submissions flooded cnn ireport over the turkey protests\n",
      "\n",
      "Tokenized article:\n",
      "cnn dear world fingers wrists sore past days could punch letters keyboard words one turkish man africa spent days scouring internet news turkish protests could share people turkey beyond poured feelings say heavy handed police response protests turkey cnn alone received 1 000 ireports turkey less week turks compelled document protest demand voices heard protests began plans replace istanbul park new development spread nationwide heavy handed crackdown police erdogan defended government handling protests saying friday government problem terms democratic demands also acknowledged police may used excessive force last week said ordered investigation rest world first time 30 years tears well happening road write words people ages races political viewpoints coming together fight notify local national media tag twitter make speak truth happening humanity less wrote ireporter ateloco post spread rapidly social media taksim square matters turks turkey nation quite savvy social media least 2 million tweets hashtags related turkish protests sent eight hours may 31 protests gathered steam study new york university revealed around 90 turkey comparison egypt main protest hashtag tweeted less 1 million times throughout country entire revolutionary period turkey like talk issues 57 using social network sites say share political views far higher percentage many european nations according pew research center arrest week 25 social media users accusations spreading false information demonstrations according country semi official anadolu agency news service sparked concerns human rights groups right freedom expression documenting disseminating information protests arduous task many social media became way holding authorities account know end already feel like robot whose main role click share button facebook one activist istanbul resident yelin bilgin told cnn days protests described protests moment resurrection although involved organizing felt could help cause sharing information wants protests lead self examination sides stop act consciously write share thoughts turkish media crisis protests renc korzay currently living homeland working construction company west african nation gabon heartfelt letter ireport opened story continues guilty letting friends get gassed arrested name freedom without guilty able keep colleagues company march taksim square straight work guilty able give people apartment shelter food eat water drink vinegar ease reaction chemicals sprayed bodies hoped writing collecting information even far away could help longer africa longer guilty became reporter quotes cameraman videos photographer images 24 hour news channel everything posted social media said istanbul writer arsevi zeynep seyran western coastal city izmir protests began describing pot pan hitter noisy protests recorded one night thoughtful essay summed energy protests many people silent happening turkey protesters afraid society things go backward said shining spotlight situation country hoped authorities would less likely retaliate involved protests said ultimately may soon tell effect protests turkey government flooded streets istanbul ankara cities one thing certain feverishly tweeting posting blogging documenting inclined stop time soon first time long time seyran said turkey much reason pride\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "print(\"\\nSANITY CHECK 1\")\n",
    "\n",
    "# Sanity check of counts_articles.\n",
    "\n",
    "number_articles = counts_articles.shape[0]\n",
    "voc_size_articles = counts_articles.shape[1]\n",
    "\n",
    "print(\"\\nHere are 10 tuples (A, W) with the number of times the word W appears in the article A:\")\n",
    "m = '     The article {0:<6} contains {1:<3} time(s) the word {2:<8}'\n",
    "\n",
    "for i in range(10):\n",
    "    \n",
    "    art = 0\n",
    "    word = 0\n",
    "    \n",
    "    while counts_articles[art, word] == 0:\n",
    "        art = random.randint(0, number_articles - 1)\n",
    "        word = random.randint(0, voc_size_articles - 1)\n",
    "        \n",
    "    for w, i in vocabulary_articles.items():\n",
    "        if i == word:\n",
    "            break\n",
    "    print(m.format(art, counts_articles[art, word],  w))\n",
    "\n",
    "\n",
    "print(\"\\nSANITY CHECK 2\")\n",
    "\n",
    "# Sanity check of tfidf_highlights.\n",
    "\n",
    "number_highlights = tfidf_highlights.shape[0]\n",
    "voc_size_highlights = tfidf_highlights.shape[1]\n",
    "\n",
    "print(\"\\nHere are 10 tuples (H, W) with the tfidf of the word W in the highlight H:\")\n",
    "m = '     The word {2:<15} in the highlight {0:<8} has a tf-idf of {1:<8}.'\n",
    "\n",
    "for i in range(10):\n",
    "    \n",
    "    high = 0\n",
    "    word = 0\n",
    "    \n",
    "    while tfidf_highlights[high, word] == 0:\n",
    "        high = random.randint(0, number_highlights - 1)\n",
    "        word = random.randint(0, voc_size_highlights - 1)\n",
    "        \n",
    "    for w, i in vocabulary_highlights.items():\n",
    "        if i == word:\n",
    "            break\n",
    "    print(m.format(high, tfidf_highlights[high, word],  w))\n",
    "\n",
    "\n",
    "print(\"\\nSANITY CHECK 3\")\n",
    "\n",
    "# Sanity check of the (article, highlight) pairs.\n",
    "pair = random.randint(0, len(pairs)-1)\n",
    "print(\"\\nThe following highlight sums up the following tokenized article:\")\n",
    "print(\"\\nHighlight:\")\n",
    "print(\" \".join(tokenized_highlights[pairs[pair][1]]))\n",
    "\n",
    "print(\"\\nTokenized article:\")\n",
    "print(\" \".join(tokenized_articles[pairs[pair][0]]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
